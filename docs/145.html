<html>
<head>
<title>Nginx Tutorial #2: Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Nginx教程#2:性能</h1>
<blockquote>原文：<a href="http://web.archive.org/web/20230307163032/https://www.netguru.com/blog/nginx-tutorial-performance#0001-01-01">http://web.archive.org/web/20230307163032/https://www.netguru.com/blog/nginx-tutorial-performance#0001-01-01</a></blockquote><div><span><div class="blog-post__lead h3"><p>你好！分享是关怀，所以我们很乐意与你分享另一个知识。我们准备了一个三部分的Nginx教程。如果你已经对Nginx有所了解，或者你只是想扩展你的经验和理解——这是你的最佳选择！</p></div><p>我们将告诉你Nginx是如何工作的，谈论它背后的概念，你如何优化它来提高你的应用程序的性能，以及如何设置它来启动和运行它。</p>
<p>本教程将包含三个部分:</p>
<ul>
<li><strong> <a href="/web/20221209135007/https://www.netguru.com/blog/nginx-tutorial-basics-concepts" rel="noopener">基础概念</a> : </strong>了解指令和上下文的区别，继承模型，以及Nginx挑选服务器块和位置的顺序。</li>
<li><strong> <a href="/web/20221209135007/https://www.netguru.com/blog/nginx-tutorial-performance" rel="noopener">性能</a> : </strong>提高速度的小窍门。我们将讨论gzip、缓存、缓冲区和超时。</li>
<li><strong> <a href="/web/20221209135007/https://www.netguru.com/blog/nginx-tutorial-ssl-setup" rel="noopener"> SSL设置</a> : </strong>设置通过HTTPS提供内容的配置。</li>
</ul>
<p>我们的目标是创建一个系列，在其中你可以很容易地找到特定主题的适当配置(比如gzip、SSL等)。)，或者干脆通读一遍。为了获得最佳的学习体验，我们建议你在自己的机器上安装<span> Nginx </span>并自己摆弄它。</p>
<h2><code>tcp_nodelay</code>、<code>tcp_nopush</code>和<code>sendfile</code></h2>
<h3><code>tcp_nodelay</code></h3>
<p>在TCP早期，工程师们面临着拥塞崩溃的危险。出现了相当多的解决方案来防止这种情况，其中之一是由John Nagle提出的算法。</p>
<p>Nagle的算法旨在防止被大量小数据包淹没。它不会干扰完整大小的TCP数据包(最大分段大小，简称MSS)，只会干扰小于MSS的数据包。只有当接收方成功发送回所有先前数据包的确认(ack)时，才会发送这些数据包。并且在等待期间，发送方可以缓冲更多的数据。</p>
<pre><code>if package.size &gt;= MSS.size
  send(package)
elsif acks.all_received?
  send(package)
else
  # accumulate data
end
</code></pre>
<p>在此期间，出现了另一个提议:延迟确认。</p>
<p>在TCP通信中，我们发送数据并接收确认(ACK ),确认告诉我们这些数据已成功传送。<br/>延迟确认试图解决网络被大量确认包淹没的问题。为了减少这个数量，接收方将等待一些数据被发送回发送方，并在这些数据中包含ack包。如果没有要发回的数据，我们必须至少每2 * MSS或每200到500 ms发送一次ack(以防我们不再收到数据包)。</p>
<pre><code>if packages.any?
  send
elsif last_ack_send_more_than_2MSS_ago? || 200_ms_timer.finished?
  send
else
  # wait
end
</code></pre>
<p>您可能已经注意到，这可能会导致持久连接上的一些临时死锁。<span>我们来重现一下吧！</span></p>
<p>假设:</p>
<ul>
<li>初始拥塞窗口等于2。拥塞窗口是另一种称为慢启动的TCP机制的一部分。细节现在并不重要，只要记住它限制了可以同时发送的包裹数量。在第一次往返中，我们被允许发送2个MSS包；第二，4个MSS包；第三，8毫秒，等等。</li>
<li>4个等待发送的缓冲包:A、B、C、D</li>
<li>a、B、C是MSS包</li>
<li>d是小包装</li>
</ul>
<p>场景:</p>
<ul>
<li>由于初始拥塞窗口大小，发送方被允许传输两个包:A和b。</li>
<li>接收器在获得两个包后发送ACK <span>。</span></li>
<li>发送方发送C包。然而，Nagle阻止他发送D(包太小，所以我们需要等待来自C的ACK)</li>
<li>在接收方，延迟的ACK阻止他发送ACK(每2个包或每200 ms发送一次)</li>
<li>200ms后，接收器发送C包的ACK。</li>
<li>发送方收到ACK，发送D包。</li>
</ul>
<p> </p>
<figure class="image&#10;    &#10;    image--framed&#10;    " data-image="" data-component="image">
    <img class="image__content" srcset="http://web.archive.org/web/20221209135007im_/https://www.netguru.com/hs-fs/hubfs/Codestories/nginx_tutorials/nagle3.png?width=700&amp;name=nagle3.png 700w, http://web.archive.org/web/20221209135007im_/https://www.netguru.com/hs-fs/hubfs/Codestories/nginx_tutorials/nagle3.png?width=2048&amp;name=nagle3.png 2048w" src="../Images/a0b8d08246d629c419dd41406bcc85cd.png" alt="nagle3.png" loading="lazy" data-image-content="" data-original-src="http://web.archive.org/web/20221209135007im_/https://www.netguru.com/hs-fs/hubfs/Codestories/nginx_tutorials/nagle3.png?width=700&amp;name=nagle3.png"/>
  </figure><br/>
<p> </p>
<p>在交换过程中，由于Nagle和延迟确认之间的死锁，引入了200毫秒的延迟。</p>
<p>Nagle算法在当时是一个真正的救星，并且仍然提供<a href="http://web.archive.org/web/20221209135007/https://news.ycombinator.com/item?id=9048947">巨大的价值</a>。然而，在大多数情况下，我们的网站不需要它，可以通过添加标志<code>TCP_NODELAY</code>安全地拒绝它。</p>
<pre><code>tcp_nodelay on;     # sets TCP_NODELAY flag, used on keepalive connections
</code></pre>
<p>享受你的200毫秒增益！</p>
<p>对于一些吹毛求疵的细节，我建议你阅读这篇伟大的论文。</p>
<h3><code>sendfile</code></h3>
<p>通常，当需要发送文件时，需要以下步骤:</p>
<ul>
<li><code>malloc(3)</code>:分配一个本地缓冲区用于存储对象数据</li>
<li><code>read(2)</code>:获取对象并复制到本地缓冲区</li>
<li><code>write(2)</code>:将对象从本地缓冲区复制到套接字缓冲区</li>
</ul>
<br/>This involves two context switches (read, write) which make a second copy of the same object unnecessary. As you may see, it is not the optimal way. Thankfully, there is another system call that improves sending files, and it's called (surprise, surprise!): <code>sendfile(2)</code>. This call retrieves an object to the file cache, and passes the pointers (without copying the whole object) straight to the socket descriptor. Netflix states that using <code>sendfile(2)</code> increased the network throughput <a href="http://web.archive.org/web/20221209135007/https://people.freebsd.org/~rrs/asiabsd_2015_tls.pdf">from 6Gbps to 30Gbps</a>.<br/><br/>However, <code>sendfile(2)</code> has some drawbacks:<br/>
<ul>
<li>不适用于UNIX套接字(例如，当通过上游服务器提供静态文件时)</li>
<li>根据操作系统的不同，性能会有所不同(此处有更多<a href="http://web.archive.org/web/20221209135007/https://blog.phusion.nl/2015/06/04/the-brokenness-of-the-sendfile-system-call/" target="_blank" rel="noopener"/>)</li>
</ul>
<p><br/>要在nginx中实现这一点，请键入:</p>
<pre>sendfile on;</pre>
<h3><code>tcp_nopush</code></h3>
<p><code>tcp_nopush</code>与<code>tcp_nodelay</code>相对。它的目标不是尽可能快地推送包裹，而是优化同时发送的数据量。<br/>在将包发送到客户端之前，它会强制包等待，直到达到其最大大小(MSS)。该指令仅在<code>sendfile</code>开启时有效。</p>
<pre>sendfile on;<br/>tcp_nopush on;</pre>
<br/>It may appear that <code>tcp_nopush</code> and <code>tcp_nodelay</code> are mutually exclusive. But if all 3 directives are turned on, nginx will:<br/>* ensure packages are full before sending them to the client<br/>* for the last packet, <code>tcp_nopush</code> will be removed, allowing TCP to send it immediately, without the 200ms delay<br/><a id="How_many_processes_should_I_have_70" data-hs-anchor="true"/>
<h2>我应该有几道工序？</h2>
<a id="Worker_processes_71" data-hs-anchor="true"/>
<h3>工作进程</h3>
<p><span/><code>worker_process</code><span>指令定义了应该运行多少工人。默认情况下，该值设置为1。最安全的设置是通过</span> <code>auto</code> <span>使用核心数。</span></p>
But still, due to Nginx's architecture, which handles requests blazingly fast, we probably won’t use more than 2-4 processes at a time (unless you are hosting Facebook, or doing some CPU-intensive stuff inside nginx).
<pre><code>worker_process auto;
</code></pre>
<a id="Worker_connections_82" data-hs-anchor="true"/>
<h3>工人连接</h3>
<p>这个指令与<code>worker_process</code>直接相关的是<code>worker_connections</code>。它指定了一个工作进程<span>可以同时打开多少个连接</span>。这个数字包括所有连接(例如，与代理服务器的连接)，而不仅仅是与客户端的连接。此外，值得记住的是，一个客户端可以打开多个连接来同时获取其他资源。</p>
<pre><code>worker_connections 1024;
</code></pre>
<a id="Open_files_limit_91" data-hs-anchor="true"/>
<h3>打开文件限制</h3>
<p>在基于Unix的系统中，“一切都是文件”。这意味着文档、目录、管道甚至套接字都是文件。系统对一个进程可以同时打开多少个文件有限制。要检查限值:</p>
<pre><code>ulimit -Sn      # soft limit
ulimit -Sn      # hard limit
</code></pre>
<p>该系统限制必须根据<code>worker_connections</code>进行调整。任何传入的连接都会打开至少一个文件(通常是双连接套接字和后端连接套接字或磁盘上的静态文件)。所以这个值等于<code>worker_connections</code> * 2是安全的。幸运的是，Nginx提供了在nginx配置中增加这个系统值的选项。为此，添加带有适当数字的<code>worker_rlimit_nofile</code>指令，并重新加载nginx。</p>
<pre><code>worker_rlimit_nofile 2048;
</code></pre>
<a id="Config_108" data-hs-anchor="true"/>
<h3>配置</h3>
<pre><code>worker_process auto;
worker_rlimit_nofile 2048; # Changes the limit on the maximum number of open files (RLIMIT_NOFILE) for worker processes.
worker_connections 1024;   # Sets the maximum number of simultaneous connections that can be opened by a worker process.
</code></pre>
<a id="Maximum_number_of_connections_118" data-hs-anchor="true"/>
<h3>最大连接数</h3>
<p>给定上述参数，我们可以计算我们可以同时处理多少个并发连接:</p>
<pre><code>max no of connections =

    worker_processes * worker_connections
----------------------------------------------
 (keep_alive_timeout + avg_response_time) * 2
</code></pre>
<p><code>keep_alive_timeout</code>(稍后将详细介绍)+ <code>avg_response_time</code>告诉我们一个连接打开了多长时间。我们还将它除以2，因为通常一个客户端会打开2个连接:一个在nginx和客户端之间，另一个在nginx和上游服务器之间。</p>
<a id="Gzip_141" data-hs-anchor="true"/>
<h2>Gzip</h2>
<p>启用<code>gzip</code>将显著降低您的响应权重，因此它将更快地出现在客户端。</p>
<a id="Compression_level_145" data-hs-anchor="true"/>
<h3>压缩级别</h3>
<p>Gzip有不同的压缩级别:从1到9。增加此级别将减小文件的大小，但也会增加资源消耗。作为标准，我们将这个数字保持在3到5之间，因为增加这个数字只会带来很小的收益，但会显著增加CPU的使用。</p>
<p>下面是一个用不同级别的gzip压缩文件的例子。0代表未压缩的文件。</p>
<pre><code>curl -I -H 'Accept-Encoding: gzip,deflate' https://www.netguru.com/
</code></pre>
<pre><code>❯ du -sh ./*
 64K    ./0_gzip
 16K    ./1_gzip
 12K    ./2_gzip
 12K    ./3_gzip
 12K    ./4_gzip
 12K    ./5_gzip
 12K    ./6_gzip
 12K    ./7_gzip
 12K    ./8_gzip
 12K    ./9_gzip

❯ ls -al
-rw-r--r--   1 matDobek  staff  61711  3 Nov 08:46 0_gzip
-rw-r--r--   1 matDobek  staff  12331  3 Nov 08:48 1_gzip
-rw-r--r--   1 matDobek  staff  12123  3 Nov 08:48 2_gzip
-rw-r--r--   1 matDobek  staff  12003  3 Nov 08:48 3_gzip
-rw-r--r--   1 matDobek  staff  11264  3 Nov 08:49 4_gzip
-rw-r--r--   1 matDobek  staff  11111  3 Nov 08:50 5_gzip
-rw-r--r--   1 matDobek  staff  11097  3 Nov 08:50 6_gzip
-rw-r--r--   1 matDobek  staff  11080  3 Nov 08:50 7_gzip
-rw-r--r--   1 matDobek  staff  11071  3 Nov 08:51 8_gzip
-rw-r--r--   1 matDobek  staff  11005  3 Nov 08:51 9_gzip
</code></pre>
<a id="gzip_http_version_11_183" data-hs-anchor="true"/>
<h3><code>gzip_http_version 1.1;</code></h3>
<p>这个指令告诉nginx仅对HTTP 1.1及以上版本使用gzip。我们这里不包括HTTP 1.0，因为对于1.0版本，不可能同时使用keepalive和gzip。你需要决定你更喜欢哪一个:HTTP 1.0客户端错过了gzip或者HTTP 1.0客户端错过了keepalive。</p>
<a id="Config_189" data-hs-anchor="true"/>
<h3>配置</h3>
<pre><code>gzip on;               # enable gzip
gzip_http_version 1.1; # turn on gzip for http 1.1 and higher
gzip_disable "msie6";  # IE 6 had issues with gzip
gzip_comp_level 5;     # inc compresion level, and CPU usage
gzip_min_length 100;   # minimal weight to gzip file
gzip_proxied any;      # enable gzip for proxied requests (e.g. CDN)
gzip_buffers 16 8k;    # compression buffers (if we exceed this value, disk will be used instead of RAM)
gzip_vary on;          # add header Vary Accept-Encoding (more on that in Caching section)

# define files which should be compressed
gzip_types text/plain;
gzip_types text/css;
gzip_types application/javascript;
gzip_types application/json;
gzip_types application/vnd.ms-fontobject;
gzip_types application/x-font-ttf;
gzip_types font/opentype;
gzip_types image/svg+xml;
gzip_types image/x-icon;
</code></pre>
<a id="Caching_222" data-hs-anchor="true"/>
<h2>贮藏</h2>
<p>缓存是另一个可以很好地为返回用户加速请求的东西。</p>
<p>管理缓存可以通过两个标题来控制:</p>
<ul>
<li><code>Cache-Control</code>用于管理HTTP/1.1中的缓存</li>
<li><code>Pragma</code>为了向后兼容HTTP/1.0客户端</li>
</ul>
<p>缓存可以分为两类:公共缓存和私有缓存。公共缓存存储响应以供多个用户重用。一个专用缓存供一个用户使用。我们可以很容易地定义应该使用哪个缓存:</p>
<pre><code>add_header Cache-Control public;
add_header Pragma public;
</code></pre>
<p>对于标准资产，我们也希望保留一个月:</p>
<pre><code>location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
  expires 1M;
  add_header Cache-Control public;
  add_header Pragma public;
}
</code></pre>
<p>上面的配置似乎已经足够了。但是，使用公共缓存时有一个注意事项。</p>
<p>让我们看看，如果我们将资产存储在公共缓存(例如CDN)中，并将URI作为唯一的标识符，会发生什么。在这个场景中，我们还假设gzip是打开的。</p>
<p>我们有两种浏览器:</p>
<ul>
<li>旧的，不支持gzip</li>
<li>一个新的，支持gzip</li>
</ul>
<p>旧浏览器向我们的CDN发送一个<code>netguru.com/style.css</code>请求。由于CDN还没有这个资源，它将向我们的服务器查询并返回未压缩的响应。CDN将文件存储在哈希中(供以后使用):</p>
<pre><code>{
  ...
  netguru.com/styles.css =&gt; FILE("/sites/netguru/style.css")
  ...
}
</code></pre>
<p>最后，文件将被返回给客户端。</p>
<p>现在，新的浏览器向CDN发送相同的请求，请求<code>netguru.com/style.css</code>，期望一个gzipped资源。由于CDN仅通过URI识别资源，它将为新浏览器返回相同的未压缩资源。新的浏览器将试图提取一个非gzip文件，并将得到垃圾。</p>
<p>如果我们可以告诉公共缓存根据URI和编码来识别资源，我们就可以避免这个问题。</p>
<pre><code>{
  ...
  (netguru.com/styles.css, gzip) =&gt; FILE("/sites/netguru/style.css.gzip")
  (netguru.com/styles.css, text/css) =&gt; FILE("/sites/netguru/style.css")
  ...
}
``
</code></pre>
<p>而这正是<code>Vary Accept-Encoding;</code>所做的。它告诉一个公共缓存，一个资源可以通过一个URI和一个<code>Accept-Encoding</code>头来区分。</p>
<p>因此，我们的最终配置如下所示:</p>
<pre><code>location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
  expires 1M;
  add_header Cache-Control public;
  add_header Pragma public;
  add_header Vary Accept-Encoding;
}
</code></pre>
<a id="Timeouts_303" data-hs-anchor="true"/>
<h2>超时设定</h2>
<p><code>client_body_timeout</code>和<code>client_header_timeout</code>定义了nginx在抛出408(请求超时)错误之前等待客户端发送主体或报头的时间。</p>
<p><code>send_timeout</code>设置向客户端发送响应的超时时间。仅在两个连续的写操作之间设置超时，而不是针对整个响应的传输。如果客户端在这段时间内没有收到任何东西，连接就会关闭。</p>
<p>设置这些值时要小心，因为太长的等待时间会使您容易受到攻击者的攻击，而太短的时间会切断慢速客户端。</p>
<pre><code># Configure timeouts
client_body_timeout   12;
client_header_timeout 12;
send_timeout          10;
</code></pre>
<a id="Buffers_327" data-hs-anchor="true"/>
<h2>缓冲</h2>
<a id="client_body_buffer_size_328" data-hs-anchor="true"/>
<h3><code>client_body_buffer_size</code></h3>
<p>设置读取客户端请求正文的缓冲区大小。如果请求体大于缓冲区，则整个请求体或仅其一部分被写入临时文件。对于<code>client_body_buffer_size</code>，在<span> <span> </span>大多数情况下</span>设置16k就足够了。</p>
<p>这是另一个可以产生巨大影响的场景，但是必须小心使用。放得太少，nginx会不断使用I/O将剩余部分写入文件。如果放得太多，当攻击者可以打开所有连接，但您无法在系统上分配一个缓冲区来处理这些连接时，您将很容易受到DOS攻击。</p>
<a id="client_header_buffer_size_and_large_client_header_buffers_336" data-hs-anchor="true"/>
<h3><code>client_header_buffer_size</code>和<code>large_client_header_buffers</code></h3>
<p>如果标题不适合<code>client_header_buffer_size</code>，那么将使用<code>large_client_header_buffers</code>。如果请求也不适合该缓冲区，则向客户端返回一个错误。对于大多数请求，1K字节的缓冲区就足够了。但是，如果一个请求包含长cookies，它可能不适合1K。</p>
<p>如果超出了请求行的大小，则向客户端返回414(请求-URI太大)错误。如果超出了请求头的大小，就会抛出400(错误请求)错误。</p>
<a id="client_max_body_size_345" data-hs-anchor="true"/>
<h3><code>client_max_body_size</code></h3>
<p>设置客户端请求正文的最大允许大小，在“内容长度”请求头字段中指定。根据您是否希望允许用户上传文件，根据您的需要调整此配置。</p>
<a id="Config_351" data-hs-anchor="true"/>
<h3>配置</h3>
<pre><code>client_body_buffer_size       16K;
client_header_buffer_size     1k;
large_client_header_buffers   2 1k;
client_max_body_size          8m;
</code></pre>
<a id="Keep_Alive_369" data-hs-anchor="true"/>
<h2>保持活力</h2>
<p>HTTP所基于的TCP协议需要执行三次握手来启动连接。这意味着在服务器向您发送数据(例如图像)之前，需要在客户端和服务器之间进行三次完整的往返。<br/>假设您从华沙请求<code>/image.jpg</code>，并连接到柏林最近的服务器:</p>
<pre><code>Open connection

TCP Handshake:
Warsaw  -&gt;------------------ synchronise packet (SYN) -----------------&gt;- Berlin
Warsaw  -&lt;--------- synchronise-acknowledgement packet (SYN-ACK) ------&lt;- Berlin
Warsaw  -&gt;------------------- acknowledgement (ACK) -------------------&gt;- Berlin

Data transfer:
Warsaw  -&gt;---------------------- /image.jpg ---------------------------&gt;- Berlin
Warsaw  -&lt;--------------------- (image data) --------------------------&lt;- Berlin

Close connection
</code></pre>
<p>对于另一个请求，您将不得不再次执行整个初始化。如果您在短时间内发送多个请求，这可能会增加很快。这就是keepalive派上用场的地方。在成功的响应之后，它保持连接空闲一段给定的时间(例如10秒)。如果在此期间发出另一个请求，将重用现有的连接并刷新空闲时间。</p>
<p>Nginx提供了一些指令，可以用来调整keepalive设置。这些可以分为两类:</p>
<ul>
<li>客户端和nginx之间保持活动状态</li>
</ul>
<pre><code>keepalive_disable msie6;        # disable selected browsers.

# The number of requests a client can make over a single keepalive connection. The default is 100, but a much higher value can be especially useful for testing with a load‑generation tool, which generally sends a large number of requests from a single client.
keepalive_requests 100000;

# How long an idle keepalive connection remains open.
keepalive_timeout 60;
</code></pre>
<ul>
<li>nginx和上游之间保持活动</li>
</ul>
<pre><code>upstream backend {
    # The number of idle keepalive connections to an upstream server that remain open for each worker process
    keepalive 16;
}

server {
  location /http/ {
    proxy_pass http://http_backend;
    proxy_http_version 1.1;
    proxy_set_header Connection "";
  }
}
</code></pre>
<p>仅此而已。</p>
<h2>结束语</h2>
<p>感谢您的阅读。如果没有在互联网深处找到的大量资源，这个系列是不可能的。在撰写本系列文章时，我们发现以下几个网站非常有用:</p>

<hr/>
<p>我们非常乐意看到一些反馈或讨论，所以请随时留下评论或以任何其他方式联系我们！你喜欢这个教程吗？关于我们下一步应该讨论什么主题，你有什么建议吗？或者你发现了一个漏洞？让我们知道，下次再见！</p></span></div>    
</body>
</html>