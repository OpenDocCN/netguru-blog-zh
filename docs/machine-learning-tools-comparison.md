# 机器学习工具比较

> 原文：<http://web.archive.org/web/20230307163032/https://www.netguru.com/blog/machine-learning-tools-comparison>

 过去几年里，机器学习蓬勃发展，而且不会很快放缓。

根据 Market and Markets 的数据，预计到 2022 年，它将从 2016 年的 10 亿美元增长到 90 亿美元。AI 在商业中的采用每月都在加速，机器学习工程师的工作也应该如此。此外，不断变化的性质

[Machine Learning](http://web.archive.org/web/20221124142404/https://www.netguru.com/services/machine-learning)

迫使机器学习工程师通过外包繁琐耗时的任务来优化他们的工作，并专注于有趣的东西。

## 优化机器学习性能

MLOps 有一个分支专注于管理工具，这些工具[使机器学习工程师能够优化他们的软件](/web/20221124142404/https://www.netguru.com/blog/whats-the-role-of-quality-assurance-specialists-in-machine-learning-projects) 的性能。从数据收集、预处理、实验处理、版本控制和部署开始的所有工作都可以在某种程度上实现自动化，而 MLOps 专注于此。这使得机器学习工程师[更加有效，并带来了混乱的解药](http://web.archive.org/web/20221124142404/https://www.netguru.com/blog/how-to-find-and-hire-good-machine-learning-data-scientists)。

然而，为项目选择最好的工具并不是一件容易的事情。这有点像工具箱——有许多工具专用于特定的任务，但在其他方面肯定会失败。想象一下，你正在为你的卧室组装一个抽屉——你肯定需要一把螺丝刀(可能还需要一些钉子)，但是凿子可能帮不上什么忙。

机器学习工具也是如此——所有的组件或多或少都是一个工具箱，它们要么非常有用，要么毫无用处。更糟糕的是，每年都有新的工具出现，这使得很难确定什么对特定情况有用。因此，关于工具的知识和清晰的比较可以帮助你做出正确的选择。

## 选择正确的机器学习工具

尽管 ML 实验有许多解决方案，但它们的使用范围变化很大，并且它们之间没有明确的比较。这篇文章将试图让你更容易选择最适合你需求的 ML 工具。这里我们将主要关注两组工具:

*   以实验跟踪为导向，
*   完全面向生命周期。

敏锐的眼睛可能会看到一个类别实际上是另一个类别的子集。事实上，实验跟踪可以成为面向全生命周期工具的一部分。然而，通常情况下，实验跟踪是最重要的功能。

## 实验性工具与面向整个生命周期的工具

根据需要，你可以选择实验跟踪工具或者面向整个生命周期的工具。就尝试不同的模型架构并为您的问题选择最佳解决方案而言，实验跟踪工具将是您的最佳选择。此外，在像 ML 概念验证这样的情况下，项目时间表很紧，在设置上损失的每一天都是痛苦的。这就是为什么一个简单(快速建立)的工具是组织和管理实验的最佳工具，以便为特定问题找到最佳解决方案[。](http://web.archive.org/web/20221124142404/https://www.netguru.com/blog/benefits-of-introducing-data-science)

如果你想采用一种工具来帮助你完成机器学习生命周期的所有步骤——从数据处理(甚至在某些情况下是数据创建)到模型部署，面向实验跟踪的工具将不会是满足你需求的最佳选择。然而，在这些解决方案中，实验跟踪功能可能非常模糊，并且被减少到最低限度。

## 面向实验跟踪的工具列表

以下是用于实验跟踪的工具列表:

### 权重和偏差

就易用性和设置时间而言，Wandb 是所有工具中的最佳选择。 [Wandb 使跟踪实验](http://web.archive.org/web/20221124142404/https://wandb.ai/site)变得极其容易，并且可以通过直观的网络界面进行比较。通过几行代码，实验在 Wandb 中得到了完整的报告。从代码的角度来看， Wandb 为您处理了大部分工作——如果您想将一些东西记录到 Wandb，您可以使用一个函数来管理它。

Wandb 的一个非常好的特点是它有在线图表——使用这些图表，用户可以快速诊断和验证实验。Wandb 也支持更高级的绘图——如果你想诊断深度学习模型的权重或其分布，Wandb 肯定是该研究的最佳解决方案。

虽然 Wandb 的主要功能是实验跟踪，但还有几个功能值得一试。报告是一个很好的工具，可以为团队和客户快速生成报告。扫描使用户能够执行超参数优化，这是一个很好的补充。Wandb 还支持工件存储。

Wandb 也是专注于发现而非交付的研究团队的首选。不用说，Wandb 是 ML insights 的好伙伴，该工具的开发似乎真的专注于帮助研究人员的工作。

如果实验跟踪是唯一需要的功能，并且预算很高，那么 Wandb 是最好的选择。然而， Wandb 只提供实验跟踪，因此如果模型部署或任何其他交互式功能(如笔记本托管)是必要的，Wandb 可能不是最佳选择。

#### 赞成的意见

*   伟大的，交互式界面。
*   最少的样板代码。
*   方便的实验比较，非常适合研究项目或验证。

#### 骗局

*   实验跟踪而已。
*   相对昂贵。

### 海王星. ai

Neptune 提供了与 Wandb 类似的体验，因为它主要专注于实验跟踪。用法类似——Neptune 提供了一个 Python 包。用户使用几行代码，首先连接到 Neptune 服务器创建一个实验，然后记录工件，这些工件存储在实验中。

Neptune 还[提供了交互式绘图](http://web.archive.org/web/20221124142404/https://neptune.ai/)——它支持各种不同的可视化库。这使得方便用户灵活选择自己喜欢的绘图包。它不提供额外的工具，如 Wandb 中的报告或扫描，但这些功能通常与实验跟踪无关，可以在代码级别上开发。

在日志记录方面，Neptune 可能没有 Wandb 那么方便，并且可能包含更多的样板代码。对常见深度学习库的自动日志记录的支持也不太先进。然而，网络界面很好且直观，并以简洁的格式提供信息，以快速发现实验结果并找到给定情况下的最佳选择。

#### 赞成的意见

*   漂亮的交互式界面。
*   相对便宜。
*   方便的实验对比。

#### 骗局

*   对于全职、长时间的项目来说可能不够。
*   仅实验跟踪。

## 面向整个生命周期的工具列表

下面是面向整个生命周期的工具列表:

### MLflow

说到实验跟踪，MLflow 的用法相当基础。如果实验涉及更复杂的可视化，MLFlow 在查看它们方面没有太大帮助。大多数图被存储为工件而不是嵌入式部件，这使得掌握实验结果变得不太方便。要查看这些图，通常需要下载并在本地研究它们。这也使得在一些实验没有带来预期结果的情况下发现问题变得相当困难。有时，可能需要大量手工工作来确定最佳方法。总有比 Wandb 或 Neptune 更多的样板代码。

然而，与之前的工具相比， [MLflow 提供了一组更丰富的特性](http://web.archive.org/web/20221124142404/https://mlflow.org/)。MLflow 的功能类似于全生命周期框架的功能。

MLflow Projects 是在 Conda 或 Docker 环境中打包代码的组件，以确保代码执行的可重复性。当代码存储库需要部署在其他地方而不仅仅是开发环境中时，这个特性非常有用。此外，MLflow Models 组件为提供了一个抽象层，用于部署来自最流行的 ML 库的机器学习模型。如果生产环境中的模型需要更新，MLFlow Models 可以轻松完成——只需几行代码，就可以部署模型进行推理。

最后，MLflow Registry 提供了模型管理——一个新的模型可以添加到模型注册表中，并通过其标识符进行访问。模型也可以被版本化，所以如果需要部署几个模型版本，模型注册中心会为我们组织它们。在紧急情况下，由于版本控制，旧模型可以恢复。

最后，MLflow 是免费的，所以如果预算紧张，这是一个不错的选择。一个经验法则是，如果你只想拥有一个实验跟踪工具，MLflow 提供了一个非常基本的特性集。如果您需要部署和模型管理功能，MLflow 可以为您提供。

#### 赞成的意见

*   免费的开源工具。
*   提供机器学习模型的管理和版本控制。
*   为机器学习模型提供部署工具。

#### 骗局

*   不容易跟踪和比较实验。

### 数据砖

Databricks 是数据工程和机器学习之间的桥梁。对于大数据解决方案，[提供数据工程工具](http://web.archive.org/web/20221124142404/https://databricks.com/)，如 Spark、创建预处理管道或统计。有一些工具可以对传入的数据进行高级分析。此外，为了完成给定的任务，可以定义可以执行的作业。例如，我们可以为新数据的探索性数据分析定义脚本，或者我们可以为数据处理定义作业。

该工具还提供了数据版本控制，这对于数据不断更新的场景非常有用。想象一个机器学习模型根据不断变化的数据进行训练的情况。如果没有版本控制，实现可复制性是困难的，并且会变成一个不可能的任务久而久之和数据增长。

Databricks 支持协作笔记本，其中可视化可以直接从新数据构建。支持 Python、SQL、R、Scala。可以安排笔记本的执行，以便可视化效果可以与输入的数据一起更新。

对于实验跟踪，Databricks 提供了管理的 MLflow 来训练数据上的模型。界面和免费的 MLflow 版本是一样的，所以对于那里的实验追踪体验并没有太大的提升。可视化是基本的，可以保存为附加到特定实验的工件。除了实验跟踪之外，MLflow 中的所有其他功能也包含在 Databricks 中。从模型注册到模型部署，所有这些特性都可以从 Databricks 接口获得。

一般来说，如果您的项目涉及大数据和复杂的预处理管道，Databricks 将是对您的项目进行端到端管理的绝佳选择。在更简单的项目中，数据不会改变太多，并且数量很少，MLflow 的开源版本肯定就足够了。

#### 赞成的意见

*   非常适合大数据项目。

#### 骗局

*   与免费的 MLflow 版本相比，ML 部分不会带来太多好处。

### 多轴

Polyaxon 是一个强大的工具，可以完成多种任务。它提供了一个在 Kubernetes 集群上运行任何作业的接口，这个集群可以由客户端设置，也可以在云中管理。因此，它可以作为机器学习项目的实验跟踪平台，也可以用于任何其他工作，如数据处理、可视化或其他分析。它可以托管笔记本服务器甚至 Visual Studio 代码。

说到实验追踪体验，Polyaxon 和 MLflow 差不多。文件和图被存储为工件，没有交互图，并且界面不关注实验比较。它更像是一个实验仓库。

然而，作为一个端到端的工具，Polyaxon 非常适合管理许多项目和团队协作。如果团队从事许多需要高计算能力或 GPU 使用的项目，Polyaxon 是一个优化分配资源的伟大平台。对于任何任务，从数据处理、训练到可视化结束，Polyaxon 都是一种节省时间和协调机器学习计算能力的好方法。

Polyaxon 的主要缺点是它的设置时间，因为 Polyaxon 在 Kubernetes 集群上工作。然而，一旦设置了 Polyaxon，后续项目可以很容易地添加，并且设置非常短。还有一个选择是[云部署](/web/20221124142404/https://www.netguru.com/services/cloud-application-development)，其中 Polyaxon 负责基础设施，但它有点贵(计划是为客户量身定制的)。

如果你在一个团队中工作，有多个项目，并且需要访问多核 CPU 或 GPU 机器，Polyaxon 可以作为管理 ML 项目的中央解决方案。不需要太多的设置，您就可以快速地从设置转移到实际工作。最重要的是，Polyaxon 提供了许多实用程序，如笔记本托管、容器化、部署、实验跟踪、机器学习模型的注册等等。如果你独自工作，或者你只需要一个轻量级的解决方案，Polyaxon 对你来说可能是多余的。

#### 赞成的意见

*   各种各样的应用。

#### 骗局

*   对于基本任务来说可能有点过了。
*   不是很好的实验跟踪功能。

### aws pagemaker

AWS SageMaker 是亚马逊提供的解决方案，用于大规模构建、训练和部署机器学习模型。使用 web 界面，可以配置数据，选择模型，并启动基础设施来训练机器学习模型。

SageMaker 为最常见的机器学习算法提供了一个现成的实现——只需要提供数据(应该存储在 S3 桶中)。然后对模型进行训练，并在 AWS 上轻松部署。可以从 AWS web 界面快速定义端点。SageMaker 以相对较低的价格租赁实验计算能力。还有一个提供定制培训程序的选项。在这种情况下，需要将带有培训过程的 Docker 容器上传到 ECR，以便执行培训。

SageMaker 还提供了许多其他工具所没有的不同特性。例如，在大量数据需要标注的情况下，AWS 提供数据标注服务——给定费用，可以雇佣数据标注员为项目准备数据。

SageMaker 与其他 AWS 组件紧密合作。这是优点，也是缺点。对于已经部署在 AWS 上的项目，生产率可以大大提高，因为 ML 解决方案可以从 AWS 提供的可重用构建模块中交付。对于涉及不同云供应商的项目，SageMaker 可能不是最佳选择。

当涉及到实验跟踪体验时，只提供来自已执行作业的日志，而没有可视化方面的交互体验。这使得对项目中的特定问题进行研究变得极其困难。一般来说，对于明确定义和已知的 ML 问题，SageMaker 是很好的，尽管在需要进行一些研究的情况下可能没有帮助。

SageMaker 提供了笔记本实例的托管，所以对于习惯在笔记本上进行实验的用户来说，SageMaker 是一个不错的选择。SageMaker 还为不同于模型训练的案例提供处理工作。例如，在需要以某种方式处理原始数据集的情况下，可以为此目的定义单个作业，并根据需要对位于特定 S3 桶中的数据执行该作业。

SageMaker 还提供机器学习模型的便捷部署。借助简单的端点配置工具，可以快速将模型部署到生产环境中，并从中进行推断。

#### 赞成的意见

*   非常适合以 AWS 为中心的项目。

#### 骗局

*   非常差的实验跟踪体验。
*   不支持不同的云供应商。

### 瓦罗海

Valohai 在某种程度上与 SageMaker、Polyaxon 和 Databricks 竞争，因为它处理机器学习项目的整个生命周期。然而，这个想法要简单得多——培训、评估、部署和重复。通过提供一个 Github 库，或者一个笔记本，或者只是一个脚本，解决方案在云中执行，结果被保存。如果必要性是快速训练模型，然后部署它- Valohai 是实现它的一种手段。为什么要用？如果没有计算能力来进行实验，瓦罗海可以提供。只需很少的努力，就可以执行实验，并且可以访问结果，或者在 Valohai 中部署结果。

这个解决方案非常适合具有不同供应商需求的团队。Valohai 与 GCP、AWS 或 Azure 配合良好，并负责这些平台的配置。Valohai 确保结果的完全可再现性，因为实验期间执行的代码存储在实验内部。

Valohai 肯定不是探索不同 ML 方法的解决方案——对于 Valohai 来说，超参数调优是最可行的探索场景。换句话说，如果目标是找一个实验追踪工具，Valohai 不会是一个好的选择。

#### 赞成的意见

*   使用简单。
*   良好的 web 界面支持。

#### 骗局

*   除了全生命周期解决方案的最低要求之外，缺少其他功能。

### Floydhub

Floydhub 在功能上与 Valohai 非常相似- [它提供计算服务器](http://web.archive.org/web/20221124142404/https://www.floydhub.com/)来运行机器学习实验，并为它们提供部署服务。它绝对不是一个实验跟踪工具。平台提供了安装了软件包的深度学习环境，这些软件包将用于运行 Python 脚本。该平台还确保结果的完全可再现性，因为执行的代码存储在实验中。

如果需要训练机器学习模型，首先需要上传数据。然后，从命令行提供数据供脚本使用，并将训练结果存储在 Floydhub 中。还可以从 Floydhub 上托管的笔记本电脑上执行培训。在特定机器学习模型准备好生产的情况下，可以定义 REST API 端点，并且可以部署该模型。

这个工具非常容易使用。使用场景是，如果缺少执行训练的计算资源，Floydhub 会提供服务来执行它。

#### 赞成的意见

*   使用简单。
*   与 Valohai 相比，Floydhub 提供笔记本支持。

#### 骗局

*   除了全生命周期解决方案的最低要求之外，缺少其他功能。

## 机器学习工具的各个方面

有许多工具可以帮助机器学习工程师的工作，其中每一个都适合特定的情况。了解和理解这些差异可能有助于选择正确的工具——无论是应该一个卓越的实验跟踪体验、一个完整的机器学习生命周期平台，还是只是一个用于运行实验的平台，选项都是可用的。在本文中，只描述了 8 种不同的工具，但是市场上有更多的工具。希望本文向您展示了在寻找 MLOps 解决方案时应该考虑哪些方面。